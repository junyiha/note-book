---
category: DeepLearning
date: 2025-01-05 09:00:00 +0800
layout: post
title: ov::Core
tag: OpenVINO
---
## 摘要

+ ov::Core 类 相关学习笔记

<!--more-->

## ov::Core core 详解 

在 OpenVINO 的 C++ API 中，`ov::Core` 是核心类，用于管理推理引擎、加载模型、编译模型以及设置设备配置等任务。它是 OpenVINO 推理流程的入口，贯穿整个推理生命周期。

---

## 1. **`ov::Core` 的概述**
- **功能**: 
  - 加载模型。
  - 查询设备信息。
  - 编译模型到特定设备。
  - 设置全局或设备特定的配置。
- **设计**: `ov::Core` 提供了一个统一的接口，支持多种硬件设备（如 CPU、GPU、VPU 等）的推理。

---

## 2. **`ov::Core` 的创建**
使用默认构造函数创建 `ov::Core` 对象。 

### 示例
```cpp
#include <openvino/openvino.hpp>

int main() {
    // 创建 Core 对象
    ov::Core core;

    return 0;
}
```

`ov::Core` 会自动加载系统中可用的设备插件。如果插件未自动加载，也可以手动加载。

---

## 3. **常用方法**
`ov::Core` 提供了一系列方法，支持模型加载、编译、配置和查询功能。

### **a. 加载模型**
从 IR 文件（`.xml` 和 `.bin` 文件）加载模型到内存：

```cpp
// 加载模型到内存
auto model = core.read_model("model.xml");
```

如果模型使用的是 ONNX 格式，也可以直接加载：

```cpp
auto model = core.read_model("model.onnx");
```

---

### **b. 编译模型**
将模型编译到指定设备（如 CPU、GPU 等）上：

```cpp
// 编译模型到 CPU
auto compiled_model = core.compile_model(model, "CPU");

// 编译模型到 GPU，并设置配置
ov::AnyMap config = {
    {ov::hint::performance_mode.name(), ov::hint::PerformanceMode::LATENCY}
};
auto compiled_model_gpu = core.compile_model(model, "GPU", config);
```

编译后的模型 (`ov::CompiledModel`) 包含推理所需的优化数据，之后可以创建推理请求。

---

### **c. 查询支持的设备**
列出系统中可用的设备：

```cpp
// 查询可用设备列表
auto available_devices = core.get_available_devices();
for (const auto& device : available_devices) {
    std::cout << "Available device: " << device << std::endl;
}
```

---

### **d. 查询设备能力**
获取设备的支持能力：

```cpp
// 查询 CPU 的支持能力
auto capabilities = core.get_property("CPU", ov::supported_properties);
for (const auto& capability : capabilities) {
    std::cout << "Capability: " << capability << std::endl;
}
```

---

### **e. 设置全局或设备特定配置**
为推理引擎或设备设置配置参数：

#### **全局配置**
```cpp
// 设置全局性能模式
core.set_property({{ov::hint::performance_mode.name(), ov::hint::PerformanceMode::THROUGHPUT}});
```

#### **设备特定配置**
```cpp
// 设置 CPU 的线程数
core.set_property("CPU", {{ov::inference_num_threads.name(), 8}});
```

---

### **f. 查询模型输入和输出信息**
可以直接查询加载的模型的输入输出信息：

```cpp
// 获取输入和输出信息
for (const auto& input : model->inputs()) {
    std::cout << "Input name: " << input.get_any_name() << std::endl;
    std::cout << "Input shape: " << input.get_shape() << std::endl;
}
for (const auto& output : model->outputs()) {
    std::cout << "Output name: " << output.get_any_name() << std::endl;
    std::cout << "Output shape: " << output.get_shape() << std::endl;
}
```

---

## 4. **`ov::Core` 的高级用法**

### **a. 手动加载插件**
如果插件未自动加载，可以指定插件路径：

```cpp
ov::Core core("/path/to/plugins");
```

### **b. 加载动态批处理模型**
支持动态输入大小的模型：

```cpp
// 设置动态输入形状
model->reshape({{"input", ov::PartialShape{-1, 3, 224, 224}}});
auto compiled_model = core.compile_model(model, "CPU");
```

### **c. 结合流式推理**
启用多流推理来提升吞吐量：

```cpp
ov::AnyMap config = {
    {ov::num_streams.name(), "AUTO"}
};
auto compiled_model = core.compile_model(model, "CPU", config);
```

---

## 5. **典型推理流程**
以下是使用 `ov::Core` 进行模型加载、编译、推理的完整流程：

```cpp
#include <openvino/openvino.hpp>

int main() {
    // 1. 创建 Core 对象
    ov::Core core;

    // 2. 加载模型
    auto model = core.read_model("model.xml");

    // 3. 编译模型到 CPU
    auto compiled_model = core.compile_model(model, "CPU");

    // 4. 创建推理请求
    auto infer_request = compiled_model.create_infer_request();

    // 5. 设置输入
    auto input_tensor = infer_request.get_input_tensor();
    // 填充输入数据
    // ...

    // 6. 执行推理
    infer_request.infer();

    // 7. 获取输出
    auto output_tensor = infer_request.get_output_tensor();
    // 处理输出数据
    // ...

    return 0;
}
```

---

## 6. **常见问题**
### **a. 插件加载失败**
- 确保插件库路径正确。
- 检查 OpenVINO 安装是否完整。

### **b. 模型输入输出不匹配**
- 检查模型的输入输出形状是否符合预期。
- 如果输入形状动态，需要先设置具体形状。

---

`ov::Core` 是 OpenVINO 的核心组件，其灵活性和功能丰富性使其适用于各种推理任务。如果你对某个具体方法或场景有疑问，可以进一步讨论！

## ov::Core::compile_model() 函数 详解

### `ov::Core::compile_model()` 函数详解

`ov::Core::compile_model()` 是 OpenVINO 的核心函数之一，用于将已加载的模型编译到特定的硬件设备（如 CPU、GPU、VPU 等）上。编译后的模型会针对目标设备进行优化，生成可直接用于推理的 `ov::CompiledModel` 对象。

---

## 1. **函数原型**

OpenVINO 的 `ov::Core` 提供多个 `compile_model` 方法的重载：

```cpp
// 编译模型到指定设备
ov::CompiledModel compile_model(const std::shared_ptr<const ov::Model>& model, 
                                const std::string& device_name, 
                                const ov::AnyMap& config = {});

// 使用文件路径编译模型
ov::CompiledModel compile_model(const std::string& model_path, 
                                const std::string& device_name, 
                                const ov::AnyMap& config = {});

// 通过现有的 CompiledModel 创建新的编译模型
ov::CompiledModel compile_model(const ov::CompiledModel& compiled_model, 
                                const ov::AnyMap& config = {});
```

---

## 2. **主要功能**
1. **模型优化**：
   - 优化模型以适应特定硬件设备，充分利用硬件资源。
   - 根据设备特性，调整模型的执行方式（如并行化、流水线化等）。
2. **设备绑定**：
   - 将模型绑定到指定的推理设备。
3. **配置应用**：
   - 根据提供的配置参数（如性能模式、线程数、批处理大小等）进一步调整模型行为。

---

## 3. **参数详解**

### **a. `model`**
- **类型**: `std::shared_ptr<const ov::Model>`
- **描述**: 表示一个已经加载到内存中的模型（使用 `ov::Core::read_model()` 加载）。
- **注意**:
  - 模型可以是动态形状或静态形状。
  - 如果是动态形状，需要在编译前调用 `reshape()` 设置具体形状。

### **b. `model_path`**
- **类型**: `std::string`
- **描述**: 直接从文件路径加载并编译模型。
- **支持的文件格式**:
  - OpenVINO IR 格式（`.xml` 和 `.bin`）。
  - ONNX 格式（`.onnx`）。

### **c. `device_name`**
- **类型**: `std::string`
- **描述**: 指定目标设备名称。
- **常见设备名称**:
  - `"CPU"`: 中央处理器。
  - `"GPU"`: 图形处理器。
  - `"MYRIAD"`: VPU（视觉处理单元）。
  - `"AUTO"`: 自动选择最优设备。
  - `"HETERO:DEVICE1,DEVICE2"`: 组合多个设备。
- **注意**:
  - 设备名称必须与系统中已安装的插件匹配。

### **d. `config`**
- **类型**: `ov::AnyMap`
- **描述**: 配置参数，用于微调模型在目标设备上的执行行为。
- **常见配置参数**:
  - `ov::hint::performance_mode`: 性能优化模式（如 `LATENCY`、`THROUGHPUT`）。
  - `ov::inference_num_threads`: 推理线程数。
  - `ov::num_streams`: 推理流数量。
  - `ov::enable_profiling`: 是否启用性能分析。

---

## 4. **返回值**
- **类型**: `ov::CompiledModel`
- **描述**: 表示已编译的模型对象，包含推理所需的优化数据。
- **用法**:
  - 创建推理请求（`ov::CompiledModel::create_infer_request()`）。
  - 查询模型的输入输出信息。

---

## 5. **使用示例**

### **基本用法**
从文件加载并编译模型到 CPU：

```cpp
#include <openvino/openvino.hpp>

int main() {
    ov::Core core;

    // 加载模型
    auto model = core.read_model("model.xml");

    // 编译模型到 CPU
    auto compiled_model = core.compile_model(model, "CPU");

    return 0;
}
```

### **使用配置优化推理**
设置性能模式为 `LATENCY`，线程数为 8：

```cpp
ov::AnyMap config = {
    {ov::hint::performance_mode.name(), ov::hint::PerformanceMode::LATENCY},
    {ov::inference_num_threads.name(), 8}
};

// 编译模型并应用配置
auto compiled_model = core.compile_model(model, "CPU", config);
```

### **动态加载和编译**
直接从文件路径编译模型到 GPU：

```cpp
auto compiled_model = core.compile_model("model.xml", "GPU");
```

### **使用动态形状**
支持动态输入形状的模型：

```cpp
// 加载模型
auto model = core.read_model("model.xml");

// 设置动态输入形状
model->reshape({{"input", ov::PartialShape{-1, 3, 224, 224}}});

// 编译模型到 CPU
auto compiled_model = core.compile_model(model, "CPU");
```

---

## 6. **高级用法**

### **编译多设备模型**
通过 `HETERO` 指定多个设备：

```cpp
auto compiled_model = core.compile_model(model, "HETERO:CPU,GPU");
```

### **自动设备分配**
通过 `AUTO` 自动选择最优设备：

```cpp
auto compiled_model = core.compile_model(model, "AUTO");
```

### **重新配置编译模型**
对现有的 `CompiledModel` 对象重新应用配置：

```cpp
auto new_config = ov::AnyMap{
    {ov::num_streams.name(), "2"}
};
auto new_compiled_model = core.compile_model(compiled_model, new_config);
```

---

## 7. **错误处理**

### **常见错误**
- **设备不可用**:
  - 确保目标设备插件已安装并加载。
  - 检查设备名称拼写是否正确。

- **配置无效**:
  - 确保传递的配置参数是目标设备支持的。
  - 使用 `ov::supported_properties` 查询支持的配置。

- **模型形状不匹配**:
  - 如果输入形状动态，确保在编译前调用 `reshape()`。

### **异常捕获**
使用 `try-catch` 捕获并处理异常：

```cpp
try {
    auto compiled_model = core.compile_model(model, "CPU");
} catch (const std::exception& e) {
    std::cerr << "Error: " << e.what() << std::endl;
}
```

---

## 8. **总结**

`ov::Core::compile_model()` 是 OpenVINO 推理工作流的关键步骤，负责优化和设备绑定。它的灵活性使得用户可以根据需求调整性能、适配硬件并微调推理行为。通过熟练使用该函数，用户可以充分利用 OpenVINO 的强大功能，提高推理效率。如果你有特定场景的需求，欢迎进一步探讨！