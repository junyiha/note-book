---
category: DeepLearning
date: 2024-12-30 09:00:00 +0800
layout: post
title: OpenVino
tag: AI部署
---
## 摘要

+ OpenVino 相关学习笔记

## ov::AnyMap 详解

在 OpenVINO 的 C++ API 中，`ov::AnyMap` 是一个通用的键值对数据结构，用于设置和传递模型编译或推理时的配置参数。它允许用户通过简单的键值映射来定义特定配置，以便调整推理过程的行为。

---

## 1. **`ov::AnyMap` 的基本概念**
- **类型**: `std::map<std::string, ov::Any>` 的别名。
- **功能**: 存储键值对，其中键为配置参数的名称，值为 `ov::Any` 类型，用于支持不同类型的参数值。
- **用途**:
  - 配置推理设备的行为（如线程数、性能模式）。
  - 设置优化参数（如批处理大小、输入精度等）。
  - 传递插件特定的自定义配置。

---

## 2. **创建和使用**
`ov::AnyMap` 是可选配置的一部分，通常在调用 `ov::Core::compile_model` 或类似方法时使用。

### **创建一个 `ov::AnyMap`**
以下是创建和设置 `ov::AnyMap` 的示例：

```cpp
#include <openvino/openvino.hpp>

int main() {
    // 创建一个配置参数的 AnyMap
    ov::AnyMap config;

    // 设置性能模式
    config[ov::hint::performance_mode.name()] = ov::hint::PerformanceMode::THROUGHPUT;

    // 设置线程数
    config[ov::inference_num_threads.name()] = 8;

    // 设置流数量
    config[ov::num_streams.name()] = "AUTO";

    return 0;
}
```

### **使用 `ov::AnyMap`**
通过 `ov::Core` 编译模型时传入 `ov::AnyMap`：

```cpp
// 加载模型
auto model = core.read_model("model.xml");

// 使用配置编译模型
auto compiled_model = core.compile_model(model, "CPU", config);
```

---

## 3. **`ov::Any` 的特性**
`ov::Any` 是一个多态类型，支持存储任意类型的值。常见的值类型包括：
- 整数 (`int`)
- 浮点数 (`float`, `double`)
- 字符串 (`std::string`)
- 枚举类型（如 `ov::hint::PerformanceMode`）
- 布尔值 (`bool`)

使用 `ov::Any` 时，自动处理值类型，例如：

```cpp
// 设置不同类型的值
ov::AnyMap config;
config["threads"] = 4; // int
config["enable_profiling"] = true; // bool
config["device"] = std::string("CPU"); // string
```

---

## 4. **常见配置参数**
以下是 OpenVINO 支持的一些常见配置参数：

| 参数名称                     | 值类型                      | 描述                               |
| ---------------------------- | --------------------------- | ---------------------------------- |
| `ov::hint::performance_mode` | `ov::hint::PerformanceMode` | 性能优化模式，如延迟或吞吐量优化。 |
| `ov::inference_num_threads`  | `int`                       | 推理线程数。                       |
| `ov::num_streams`            | `std::string`               | 流数量，如 "AUTO"、"1"、"2" 等。   |
| `ov::enable_profiling`       | `bool`                      | 是否启用性能分析。                 |
| `ov::hint::model_priority`   | `ov::hint::Priority`        | 模型优先级设置。                   |

---

## 5. **如何读取配置**
使用 `ov::AnyMap` 设置参数后，可以通过以下方式访问参数值：

```cpp
// 读取配置值
auto threads = config["threads"].as<int>(); // 获取 int 类型的线程数
auto enable_profiling = config["enable_profiling"].as<bool>(); // 获取 bool 类型的值
```

---

## 6. **错误处理**
- 如果尝试访问不存在的键，会抛出异常。
- 如果类型转换失败（如尝试将 `bool` 转换为 `int`），会引发类型错误。

示例：

```cpp
try {
    auto value = config["non_existing_key"].as<int>();
} catch (const std::exception& e) {
    std::cerr << "Error: " << e.what() << std::endl;
}
```

---

## 7. **结合设备插件**
`ov::AnyMap` 可以传递设备插件特定的配置。例如，针对 GPU：

```cpp
ov::AnyMap gpu_config = {
    {"CACHE_DIR", "/path/to/cache"}, // GPU 缓存目录
    {ov::hint::performance_mode.name(), ov::hint::PerformanceMode::THROUGHPUT}
};
auto compiled_model = core.compile_model(model, "GPU", gpu_config);
```

---

### 总结
`ov::AnyMap` 是 OpenVINO 中灵活且强大的配置工具，用户可以根据需要动态设置和调整推理的行为，优化性能。结合设备特性和推理场景，灵活使用 `ov::AnyMap` 是提升模型部署效率的重要手段。如果需要针对某个配置参数的具体说明，请进一步告知！

## C++ OpenVINO ov::hint::performance_mode 详解

在 OpenVINO 的 C++ API 中，`ov::hint::performance_mode` 是一个用于优化推理性能的设置选项。它允许用户为推理过程选择不同的性能模式，以平衡吞吐量、延迟和设备资源利用率等方面的需求。

以下是对 `ov::hint::performance_mode` 的详解：

---

## 1. **性能模式的类型**
`ov::hint::performance_mode` 支持以下几种模式，每种模式都有其适用场景：

### **a. `LATENCY`**
- **描述**: 优化以降低单次推理的延迟。
- **应用场景**:
  - 实时应用，如视频分析和在线推理。
  - 延迟敏感的场景。

### **b. `THROUGHPUT`**
- **描述**: 优化以提高设备的整体吞吐量，通常通过增加批处理大小和并行化执行。
- **应用场景**:
  - 离线批量处理。
  - 批量推理任务，如图像分类。

### **c. `CUMULATIVE_THROUGHPUT`**
- **描述**: 专注于在多次推理任务中累积吞吐量的优化。
- **应用场景**:
  - 多模型部署。
  - 并发推理任务的优化。

### **d. `UNDEFINED`**
- **描述**: 使用插件的默认性能配置，通常适用于简单场景或不确定优化需求时。
- **应用场景**:
  - 开发阶段。
  - 初始实验。

---

## 2. **设置方式**
通过 `ov::Core` 或 `ov::CompiledModel` 对象设置性能模式。以下是示例代码：

```cpp
#include <openvino/openvino.hpp>

int main() {
    // 创建 OpenVINO Core 对象
    ov::Core core;

    // 加载模型
    auto model = core.read_model("model.xml");

    // 配置性能模式
    ov::AnyMap config = {
        {ov::hint::performance_mode, ov::hint::PerformanceMode::LATENCY}
    };

    // 编译模型并应用配置
    auto compiled_model = core.compile_model(model, "CPU", config);

    // 执行推理
    auto infer_request = compiled_model.create_infer_request();
    // 推理逻辑...
    return 0;
}
```

---

## 3. **性能模式的影响**
性能模式主要通过以下方式影响推理行为：
- **线程优化**: 调整计算线程数、绑定策略等。
- **批量处理**: 增加或减少批处理大小。
- **并行化策略**: 优化并行计算的粒度和线程分配。

---

## 4. **最佳实践**
- 在实时系统中，优先选择 `LATENCY`。
- 离线任务或批量处理，选择 `THROUGHPUT`。
- 如果部署多个模型，选择 `CUMULATIVE_THROUGHPUT`。
- 开发和测试阶段，可以先使用 `UNDEFINED`，然后根据实际需求调整。

---

## 5. **设备支持**
不同的设备插件（如 CPU、GPU、VPU 等）可能对性能模式的支持有细微差异。例如：
- **CPU**: 支持所有性能模式，并提供多线程优化。
- **GPU**: 注重吞吐量优化，但延迟模式下可能受到硬件约束。
- **VPU**: 通常用于低功耗场景，优化吞吐量和延迟的权衡。

了解设备特性和任务需求，有助于更好地选择性能模式。

---

如果你有具体问题或需要针对某个场景的配置示例，请告诉我！

## OpenVINO windows下环境配置教程

+ [OpenVINO 安装指南](https://docs.openvino.ai/2024/get-started/install-openvino/install-openvino-archive-windows.html)

## OpenVino 是什么

OpenVINO（Open Visual Inference and Neural Network Optimization）是一个由 **Intel** 提供的开源工具包，旨在加速深度学习模型的推理和优化，特别是在 Intel 硬件（如 CPU、GPU、FPGA 和 VPU）上的性能表现。它主要用于计算机视觉应用，但也支持其他类型的 AI 工作负载。

### 主要功能
1. **模型优化**  
   OpenVINO 提供了模型优化器（Model Optimizer），可以将各种深度学习框架（如 TensorFlow、PyTorch、ONNX、Caffe 等）训练的模型转换为 OpenVINO 的中间表示格式（IR Format）。这可以帮助简化模型并提升推理效率。

2. **硬件加速**  
   支持在多种 Intel 硬件上运行，包括：  
   - Intel CPU（尤其是使用了 AVX 和其他指令集优化）  
   - Intel 集成显卡（GPU）  
   - Intel Movidius VPU（视觉处理单元）  
   - Intel FPGA（可编程逻辑门阵列）

3. **高效推理引擎**  
   OpenVINO 提供了推理引擎，可以通过异构计算同时利用多种硬件的优势，最大化推理性能。

4. **预训练模型和工具支持**  
   包括预训练的模型库（Open Model Zoo），以及一些便捷的 API 和开发工具，用于快速开发 AI 应用。

5. **跨平台支持**  
   OpenVINO 支持在各种操作系统上运行，包括 Linux、Windows 和 macOS。

### 使用场景
- **计算机视觉**：如目标检测、图像分类、语义分割、人脸识别等。
- **视频分析**：如实时监控、行为分析、视频内容理解等。
- **自然语言处理（NLP）**：尽管主要面向视觉任务，但 OpenVINO 也支持 NLP 模型的优化和推理。
- **嵌入式 AI**：在资源受限的设备（如边缘设备）上运行优化的 AI 模型。

### 优势
1. 提高推理速度，降低延迟。
2. 支持多种硬件，灵活适应不同的部署需求。
3. 开源且有丰富的文档和社区支持。
4. 提供易用的 API，适合快速开发。

如果你正在开发需要部署深度学习模型的应用，并且目标硬件是 Intel 平台，OpenVINO 是一个非常合适的选择。

## OpenVino 推理框架 详解

OpenVINO 的推理框架（Inference Framework）是 OpenVINO 工具包的核心组件之一，它负责高效地执行优化后的深度学习模型推理。以下是 OpenVINO 推理框架的详细解析：

---

### **1. 核心组件**
#### **(1) 模型优化器（Model Optimizer）**
- 将训练好的深度学习模型转换为 OpenVINO 的中间表示（Intermediate Representation，IR）格式。
- **输入支持：**
  - 支持多种框架的模型，包括 TensorFlow、PyTorch、ONNX、Caffe 等。
- **优化方式：**
  - 图形优化（删除冗余运算节点）。
  - 数据类型精简（如 FP32 转换为 FP16）。
  - 插入专为 Intel 硬件设计的高效计算指令。

#### **(2) 推理引擎（Inference Engine）**
- 核心执行模块，负责加载优化后的 IR 模型并在目标硬件上高效运行。
- **功能：**
  - 提供统一的 API 接口，支持多种硬件。
  - 实现同步和异步推理（适用于实时和离线场景）。
  - 自动分配推理负载到多个设备（如 CPU 和 GPU 同时工作）。
- **主要模块：**
  - **核心对象（Core API）：** 用于加载模型和配置设备。
  - **插件架构：** 针对不同硬件有特定插件，如 CPU 插件、GPU 插件、VPU 插件。

#### **(3) 硬件支持层（Hardware Abstraction Layer）**
- 提供针对 Intel 硬件优化的指令集（如 AVX、VNNI）。
- 支持的设备包括：
  - CPU（使用线程优化和向量化技术）。
  - GPU（集成 GPU 的 OpenCL 加速）。
  - VPU（Movidius 芯片）。
  - FPGA（灵活可编程硬件）。

---

### **2. 工作流程**
1. **模型准备**
   - 从训练框架导出模型（如 `.pb`、`.onnx`、`.caffemodel`）。
   - 使用模型优化器将模型转换为 IR 格式（包括 `.xml` 和 `.bin` 文件）。
2. **加载模型**
   - 使用推理引擎的 Core API 加载 IR 模型到指定设备。
3. **预处理**
   - 调整输入数据的格式（如大小、数据类型、归一化等）。
4. **推理执行**
   - 调用推理引擎执行推理操作（支持同步或异步）。
5. **后处理**
   - 对推理结果进行解码或进一步处理（如 NMS、标签映射等）。

---

### **3. 编程接口**
#### **(1) Python API**
- 简单易用，适合快速开发和原型验证。
- 样例代码：
  ```python
  from openvino.runtime import Core

  # 初始化 Core
  core = Core()

  # 加载模型
  model = core.read_model(model="model.xml")
  compiled_model = core.compile_model(model=model, device_name="CPU")

  # 推理
  input_data = ...  # 预处理后的输入数据
  results = compiled_model([input_data])
  ```

#### **(2) C++ API**
- 性能更高，适合生产级部署。
- 样例代码：
  ```cpp
  #include <openvino/openvino.hpp>

  int main() {
      ov::Core core;

      // 加载模型
      auto model = core.read_model("model.xml");
      auto compiled_model = core.compile_model(model, "CPU");

      // 推理
      ov::InferRequest infer_request = compiled_model.create_infer_request();
      infer_request.set_input_tensor(...);  // 设置输入
      infer_request.infer();
      auto output = infer_request.get_output_tensor();
      return 0;
  }
  ```

---

### **4. 支持的推理模式**
#### **(1) 同步推理**
- 简单、顺序执行推理。
- 适合离线处理场景。

#### **(2) 异步推理**
- 非阻塞模式，可以在等待推理结果时并行执行其他任务。
- 适合实时场景，如视频流分析。

---

### **5. 优化特性**
#### **(1) 混合精度推理**
- 支持 FP32、FP16 和 INT8 的混合精度计算。
- 使用 INT8 精度进一步提升性能，同时减少计算和内存开销。

#### **(2) 多设备推理**
- 使用 Heterogeneous Plugin 实现负载均衡。
- 自动调度任务到最佳设备。

#### **(3) 自定义层扩展**
- 支持用户定义的自定义操作，适配特定模型需求。

---

### **6. 应用场景**
1. **实时视频分析**（如安防监控、人流统计）。
2. **工业自动化**（如缺陷检测、机器人视觉）。
3. **医疗影像处理**（如肿瘤检测）。
4. **嵌入式 AI**（如无人机、智能家居设备）。

---

### **7. 常用工具**
#### **(1) Open Model Zoo**
- 提供大量预训练模型，如 ResNet、YOLO、SSD。
- 方便快速测试和验证。

#### **(2) Benchmark Tool**
- 测试模型在目标硬件上的性能。

#### **(3) Post-Training Optimization Toolkit (POT)**
- 支持后量化（Post-training Quantization），提升推理性能。

---

### **总结**
OpenVINO 推理框架通过模型优化器和推理引擎，实现高效的深度学习模型推理。其支持多种硬件设备，并提供丰富的优化选项，适用于多种 AI 应用场景，是一个功能强大、灵活且高效的推理工具。